{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building the Linear Regression Model from Scratch.\n",
    "\n",
    "In this notebook we I will be building the linear regression model from scratch along with some of its opitmization algorithms.\n",
    "\n",
    "These algorithms include:\n",
    "\n",
    "```\n",
    "* Gradient Descent (Batch Gradient Descent)\n",
    "* Stocahtic Gradient Descent\n",
    "```\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What is Linear Regression?\n",
    "\n",
    "Linear Regression predicts a target variable y fitting a straight line to data points, remeber the equation of a line?\n",
    "$$y = mx + c$$\n",
    "\n",
    "Mathematically we represent the model it as: \n",
    "\n",
    "$$h_\\theta(X) = \\theta_0 + \\theta_1X$$\n",
    "\n",
    "where:\n",
    "- $h_\\theta(X)$ is the hypothesis function\n",
    "- $\\theta_0$ is the intercept term (bias)\n",
    "- $\\theta_1$ is the slope coefficient\n",
    "- $x$ is the input variable\n",
    "\n",
    "In the equation above our data $X$ only has one feature,\n",
    "\n",
    "For multi-dimensional data $X$ with $n$ dimensions,\n",
    "$$\n",
    "X = \\begin{pmatrix}\n",
    "X_1 \\\\\n",
    "X_2 \\\\\n",
    "X_3 \\\\\n",
    "\\vdots \\\\\n",
    "X_n\n",
    "\\end{pmatrix}\n",
    "$$\n",
    "\n",
    "The hypothesis function can be rewritten as,\n",
    "$$h_\\theta(X) = \\theta_0 + \\theta_1X + \\theta_2X + \\theta_3X + ... + \\theta_nX$$\n",
    "\n",
    "Our data is in the form $(X,y)$ is one training example\n",
    "\n",
    "where: \n",
    "- $X$ is the input (feature)\n",
    "- $y$ is the output (target value I want to predict)\n",
    "\n",
    "Therefore $(X,y)$ is one training example and $(X^{(i)} ,y^{(i)})$ denote the $i^{th}$ training example\n",
    "\n",
    "#### Goal for Linear Regression \n",
    "\n",
    "The purpose of linear regression is to find a line (or hyperplane) that best fits the data points. Mathematically, this means finding the parameters $({\\theta_0, ... ,\\theta_n)}$ that minimise the discrepancy between the predicted values $({h_\\theta ({X})})$ and actual values $({y})$\n",
    "\n",
    "#### What is Discrepancy?\n",
    "\n",
    "Discrepancy is the difference between the predicted values $h_\\theta ({X})$ and the actual values $y$\n",
    "\n",
    "\n",
    "$Error$ $for$ $each$ $data$ $point$ $=  h_\\theta ({X^{(i)}}) - y^{(i)}$ \n",
    "\n",
    "\n",
    "$Squared$ $Error = {(h_\\theta ({X^{(i)}}) - y^{(i)})}^2$\n",
    "\n",
    "\n",
    "$Mean$ $Squared$ $Error$ $(MSE)$ $= \\frac{1}{m}\\sum_{i=1}^{m} {(h_\\theta ({X^{(i)}}) - y^{(i)})}^2$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Cost Function $J{(\\theta)}$\n",
    "\n",
    "For optimization purposes, we add a factor $\\frac{1}{2}$ to simplify the derivative calculations:\n",
    "$$ J{(\\theta)} = \\frac{1}{2m}\\sum_{i=1}^{m} {(h_\\theta ({X^{(i)}}) - y^{(i)})}^2$$\n",
    "\n",
    "- The $\\frac{1}{2}$ is optional but standard in machine learning. It ensures that when we complete the gradient (derivative of $J{(\\theta)}$), the factor 2 from squaring cancels out.\n",
    "\n",
    "All of this just means the goal of our the Linear Regression model is to minize this cost function $J{(\\theta)}$. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Gradient Descent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
